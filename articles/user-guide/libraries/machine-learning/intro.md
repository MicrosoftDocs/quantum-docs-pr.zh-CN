---
title: 量子机器学习库
description: 了解如何在量程系统上使用机器学习
author: alexeib2
ms.author: alexeib
ms.date: 11/22/2019
ms.topic: article
uid: microsoft.quantum.libraries.machine-learning.intro
no-loc:
- Q#
- $$v
ms.openlocfilehash: 9f7f892fb2b76432942c86163497c22f0c73d51f
ms.sourcegitcommit: 9b0d1ffc8752334bd6145457a826505cc31fa27a
ms.translationtype: MT
ms.contentlocale: zh-CN
ms.lasthandoff: 09/21/2020
ms.locfileid: "90833805"
---
# <a name="introduction-to-quantum-machine-learning"></a><span data-ttu-id="232ac-103">量程机器学习简介</span><span class="sxs-lookup"><span data-stu-id="232ac-103">Introduction to Quantum Machine Learning</span></span>

## <a name="framework-and-goals"></a><span data-ttu-id="232ac-104">框架和目标</span><span class="sxs-lookup"><span data-stu-id="232ac-104">Framework and goals</span></span>

<span data-ttu-id="232ac-105">量程编码和信息处理是传统机器学习量程分类器的一种强大替代方法。</span><span class="sxs-lookup"><span data-stu-id="232ac-105">Quantum encoding and processing of information is a powerful alternative to classical machine learning Quantum classifiers.</span></span> <span data-ttu-id="232ac-106">具体而言，它使我们能够对量程寄存器中的数据进行编码，这种方式相对于功能的数量非常简单，并将量程牵连用作计算资源并采用类推理的量程度量。</span><span class="sxs-lookup"><span data-stu-id="232ac-106">In particular, it allows us to encode data in quantum registers that are concise relative to the number of features, systematically employing quantum entanglement as computational resource and employing quantum measurement for class inference.</span></span>
<span data-ttu-id="232ac-107">以环路为中心的量程分类器是一个相对简单的量程解决方案，它将数据编码与快速 entangling/disentangling 量程线路结合起来，并使用度量来推断数据示例的类标签。</span><span class="sxs-lookup"><span data-stu-id="232ac-107">Circuit centric quantum classifier is a relatively simple quantum solution that combines data encoding with a rapidly entangling/disentangling quantum circuit followed by measurement to infer class labels of data samples.</span></span>
<span data-ttu-id="232ac-108">其目标是确保使用的是传统的主题线路和存储，还可以确保线路参数的混合量子/经典定型，甚至是非常大的功能空间。</span><span class="sxs-lookup"><span data-stu-id="232ac-108">The goal is to ensure classical characterization and storage of subject circuits, as well as hybrid quantum/classical training of the circuit parameters even for extremely large feature spaces.</span></span>

## <a name="classifier-architecture"></a><span data-ttu-id="232ac-109">分类器体系结构</span><span class="sxs-lookup"><span data-stu-id="232ac-109">Classifier architecture</span></span>

<span data-ttu-id="232ac-110">分类是一种监督的机器学习任务，其中的目标是推断类标签 $ \{ y_1、y_2、\ldots、y_d \} $ 的某些数据示例。</span><span class="sxs-lookup"><span data-stu-id="232ac-110">Classification is a supervised machine learning task, where the goal is to infer class labels $\{y_1,y_2,\ldots,y_d\}$ of certain data samples.</span></span> <span data-ttu-id="232ac-111">"定型数据集" 是示例 $ \mathcal{D} = \{ (x，y) } $ 的集合，其中包含已知的预分配标签。</span><span class="sxs-lookup"><span data-stu-id="232ac-111">The "training data set" is a collection of samples $\mathcal{D}=\{(x,y)}$ with known pre-assigned labels.</span></span> <span data-ttu-id="232ac-112">此处 $x $ 是数据样本，并且 $y $ 是其称为 "定型标签" 的已知标签。</span><span class="sxs-lookup"><span data-stu-id="232ac-112">Here $x$ is a data sample and $y$ is its known label called "training label".</span></span>
<span data-ttu-id="232ac-113">类似于传统方法，量程分类包含三个步骤：</span><span class="sxs-lookup"><span data-stu-id="232ac-113">Somewhat similar to traditional methods, quantum classification consists of three steps:</span></span>
- <span data-ttu-id="232ac-114">数据编码</span><span class="sxs-lookup"><span data-stu-id="232ac-114">data encoding</span></span>
- <span data-ttu-id="232ac-115">分类器状态的准备工作</span><span class="sxs-lookup"><span data-stu-id="232ac-115">preparation of a classifier state</span></span>
- <span data-ttu-id="232ac-116">由于度量值的概率，需要多次重复这三个步骤。</span><span class="sxs-lookup"><span data-stu-id="232ac-116">measurement Due to the probabilistic nature of the measurement, these three steps must be repeated multiple times.</span></span> <span data-ttu-id="232ac-117">分类器状态的编码和计算都是通过 *量程线路*来完成的。</span><span class="sxs-lookup"><span data-stu-id="232ac-117">Both the encoding and the computing of the classifier state are done by means of *quantum circuits*.</span></span> <span data-ttu-id="232ac-118">尽管编码线路通常是数据驱动的和无参数的，但分类器线路包含一组足够的 learnable 参数。</span><span class="sxs-lookup"><span data-stu-id="232ac-118">While the encoding circuit is usually data-driven and parameter-free, the classifier circuit contains a sufficient set of learnable parameters.</span></span> 

<span data-ttu-id="232ac-119">在建议的解决方案中，分类器线路由单 qubit 旋转和两 qubit 控制旋转组成。</span><span class="sxs-lookup"><span data-stu-id="232ac-119">In the proposed solution the classifier circuit is composed of single-qubit rotations and two-qubit controlled rotations.</span></span> <span data-ttu-id="232ac-120">此处的 learnable 参数为旋转角度。</span><span class="sxs-lookup"><span data-stu-id="232ac-120">The learnable parameters here are the rotation angles.</span></span> <span data-ttu-id="232ac-121">对于量程计算，旋转和控制旋转入口已知为 *通用* ，这意味着任何单一权重矩阵都可以分解为包含此类入口的足够长的线路。</span><span class="sxs-lookup"><span data-stu-id="232ac-121">The rotation and controlled rotation gates are known to be *universal* for quantum computation, which means that any unitary weight matrix can be decomposed into a long enough circuit consisting of such gates.</span></span>

<span data-ttu-id="232ac-122">在建议的版本中，只支持后跟单频率估算的一条线路。</span><span class="sxs-lookup"><span data-stu-id="232ac-122">In the proposed version, only one circuit followed by a single frequency estimation is supported.</span></span>
<span data-ttu-id="232ac-123">因此，解决方案是具有低度多项式内核的支持向量计算机的量程模拟。</span><span class="sxs-lookup"><span data-stu-id="232ac-123">Thus, the solution is a quantum analog of a support vector machine with a low-degree polynomial kernel.</span></span>

![多层感知器与以线路为中心的分类器](~/media/DLvsQCC.png)

<span data-ttu-id="232ac-125">可以将简单的量程分类器设计与传统的支持向量计算机进行比较 (SVM) 解决方案。</span><span class="sxs-lookup"><span data-stu-id="232ac-125">A simple quantum classifier design can be compared to a traditional support vector machine (SVM) solution.</span></span> <span data-ttu-id="232ac-126">使用最佳内核窗体 $ \sum \ alpha_j k (x_j，x) $，其中 $k $ 是特定内核函数，SVM 的数据 $x 示例的推理。</span><span class="sxs-lookup"><span data-stu-id="232ac-126">The inference for a data sample $x$ in case of SVM is done using an optimal kernel form $\sum \alpha_j  k(x_j,x)$ where $k$ is a certain kernel function.</span></span>

<span data-ttu-id="232ac-127">与此相反，量程分类器使用预测器 $p (y │ x，U ( \theta) # B3 = 〈 U ( \theta) x |M |U ( \theta) x 〉 $，这在精神上类似，但在技术上非常不同。</span><span class="sxs-lookup"><span data-stu-id="232ac-127">By contrast, a quantum classifier uses the predictor $p(y│x,U(\theta))=〈U(\theta)x|M|U(\theta)x〉$, which is similar in spirit but technically quite different.</span></span> <span data-ttu-id="232ac-128">因此，在使用简单的幅度编码时，$p (y │ x，U ( \theta) # B3 $ 是 $x $ 的 amplitudes 中的二次窗体，但不再单独了解此窗体的系数;它们改为从线路的矩阵元素聚合 $U ( \theta) $，通常，learnable 参数 $ \theta $ 比向量 $x $ 的维度少很多。</span><span class="sxs-lookup"><span data-stu-id="232ac-128">Thus, when a straightforward amplitude encoding is used,  $p(y│x,U(\theta))$ is a quadratic form in the amplitudes of $x$, but the coefficients of this form are no longer learned independently; they are instead aggregated from the matrix elements of the circuit $U(\theta)$, which typically has significantly fewer learnable parameters $\theta$ than the dimension of the vector $x$.</span></span> <span data-ttu-id="232ac-129">通过在 $l $ $x $ 个副本上使用量程产品编码，可以将原始功能中 $p (y │ x、U ( \theta) # B3 $ 的多项式度提高到 $ 2 ^ l $。</span><span class="sxs-lookup"><span data-stu-id="232ac-129">The polynomial degree of $p(y│x,U(\theta))$ in the original features can be increased to $2^l$ by using a quantum product encoding on $l$ copies of $x$.</span></span>

<span data-ttu-id="232ac-130">这种体系结构探讨了相对浅的线路，因此必须 *快速 entangling* 才能捕获所有范围内数据功能之间的所有相关性。</span><span class="sxs-lookup"><span data-stu-id="232ac-130">Our architecture explores relatively shallow circuits, which therefore must be *rapidly entangling* in order to capture all the correlations between the data features at all ranges.</span></span> <span data-ttu-id="232ac-131">下图显示了最有用的快速 entangling 线路组件示例。</span><span class="sxs-lookup"><span data-stu-id="232ac-131">An example of the most useful rapidly entangling circuit component is shown on figure below.</span></span> <span data-ttu-id="232ac-132">尽管具有此几何的线路仅包含 $3 n + 1 $ 入口，但它计算的单一权重矩阵可确保 $ 2 ^ n $ 功能之间的明显交叉对话。</span><span class="sxs-lookup"><span data-stu-id="232ac-132">Even though a circuit with this geometry consists of only $3 n+1$ gates, the unitary weight matrix that it computes ensures significant cross-talk between $2^n$ features.</span></span>

![快速 entangling 5 qubits (上的量子线路，) 两个循环层。](~/media/5-qubit-qccc.png)

<span data-ttu-id="232ac-134">上述示例中的线路包含6个 qubit 入口 $ (G_1，\ldots，G_5;G_ {16}) $ 和 10 2-qubits 入口 $ (G_6，\ldots，G_ {15}) $。</span><span class="sxs-lookup"><span data-stu-id="232ac-134">The circuit in the above example consists of 6 single-qubit gates $(G_1,\ldots,G_5; G_{16})$ and 10 two-qubits gates $(G_6,\ldots,G_{15})$.</span></span> <span data-ttu-id="232ac-135">假设每个入口都是使用一个 learnable 参数定义的，我们有16个 learnable 参数，而 5 qubit Hilbert 空间的维度为32。</span><span class="sxs-lookup"><span data-stu-id="232ac-135">Assuming that each of the gates is defined with one learnable parameter we have 16 learnable parameters, while the dimension of the 5-qubit Hilbert space is 32.</span></span> <span data-ttu-id="232ac-136">当 $n $ 为奇数时，可以轻松地将此类线路几何通用化为任何 $n 的 qubit 寄存器，并为 $ 2 ^ n $ 维特征空间产生 $3 n + 1 $ 参数的线路。</span><span class="sxs-lookup"><span data-stu-id="232ac-136">Such circuit geometry can be easily generalized to any $n$-qubit register, when $n$ is odd, yielding circuits with $3 n+1$ parameters for $2^n$-dimensional feature space.</span></span>

## <a name="classifier-training-as-a-supervised-learning-task"></a><span data-ttu-id="232ac-137">作为监督式学习任务的分类器培训</span><span class="sxs-lookup"><span data-stu-id="232ac-137">Classifier training as a supervised learning task</span></span>

<span data-ttu-id="232ac-138">分类器模型的定型涉及到查找其操作参数的最佳值，从而最大程度地提高在定型样本中推断正确训练标签的平均可能性。</span><span class="sxs-lookup"><span data-stu-id="232ac-138">Training of a classifier model involves finding optimal values of its operational parameters, such that they maximize the average likelihood of inferring the correct training labels across the training samples.</span></span>
<span data-ttu-id="232ac-139">在这里，我们只关心两种级别的分类，即 $d = $2 的情况，只有两个标签 $y _1，y_2 $。</span><span class="sxs-lookup"><span data-stu-id="232ac-139">Here, we concern ourselves with two level classification only, i.e. the case of $d=2$ and only two classes with the labels $y_1,y_2$.</span></span>

> [!NOTE]
> <span data-ttu-id="232ac-140">将方法通用化为任意数量的类的一种原则性方法是将 qubits 替换为 qudits，即，将替换为 $d $ basis 状态，并使用双向度量来 $d $-双向度量。</span><span class="sxs-lookup"><span data-stu-id="232ac-140">A principled way of generalizing our methods to arbitrary number of classes is to replace qubits with qudits, i.e. quantum units with $d$ basis states, and the two-way measurement with $d$-way measurement.</span></span>

### <a name="likelihood-as-the-training-goal"></a><span data-ttu-id="232ac-141">作为定型目标的可能性</span><span class="sxs-lookup"><span data-stu-id="232ac-141">Likelihood as the training goal</span></span>

<span data-ttu-id="232ac-142">给定一个 learnable 量程线路 $U ( \theta) $，其中 $ \theta $ 是参数的一个向量，并通过 $M $ 来表示最终度量值，正确标签推理的平均可能性是 $ $ \begin{align} \mathcal{L} ( \theta) = \frac {1} {| \mathcal{D} |} \left ( \ sum_ { (x，y_1) \in\mathcal{d}} (Y_1 M = |U ( \theta) x) + \ sum_ { (x，y_2) \in\mathcal{D}} P (M = y_2 |U ( \theta) x) \right) \end{align} $ $，其中 $P (M = y | z) $ 是在量程状态 $y $ 的度量的概率。</span><span class="sxs-lookup"><span data-stu-id="232ac-142">Given a learnable quantum circuit $U(\theta)$, where $\theta$ is a vector of parameters, and denoting the final measurement by $M$, the average likelihood of the correct label inference is $$ \begin{align} \mathcal{L}(\theta)=\frac{1}{|\mathcal{D}|} \left( \sum_{(x,y_1)\in\mathcal{D}} P(M=y_1|U(\theta) x) + \sum_{(x,y_2)\in\mathcal{D}} P(M=y_2|U(\theta) x)\right) \end{align} $$ where $P(M=y|z)$ is the probability of measuring $y$ in quantum state $z$.</span></span>
<span data-ttu-id="232ac-143">在这里，它后缀了解到 $ \theta $) $ \theta $ 的 ( 可能性，并且它的 theta_j 派生方式实质上是与用于计算可能性函数本身相同的量程协议。</span><span class="sxs-lookup"><span data-stu-id="232ac-143">Here, it suffices to understand that the likelihood function $\mathcal{L}(\theta)$ is smooth in $\theta$ and its derivative in any $\theta_j$ can be computed by essentially the same quantum protocol as used for computing the likelihood function itself.</span></span> <span data-ttu-id="232ac-144">这允许按梯度下降量优化 $ \mathcal{L} ( \theta) $。</span><span class="sxs-lookup"><span data-stu-id="232ac-144">This allows for optimizing the $\mathcal{L}(\theta)$ by gradient descent.</span></span>

### <a name="classifier-bias-and-training-score"></a><span data-ttu-id="232ac-145">分类器偏差和定型分数</span><span class="sxs-lookup"><span data-stu-id="232ac-145">Classifier bias and training score</span></span>

<span data-ttu-id="232ac-146">如果 $ \theta $ 中的参数有一些中间 (或最终) 值，则需要确定单个实数值 $b $ 称为 *分类器偏差* 来执行推理。</span><span class="sxs-lookup"><span data-stu-id="232ac-146">Given some intermediate (or final) values of the parameters in $\theta$, we need to identify a single real value $b$ know as *classifier bias* to do the inference.</span></span> <span data-ttu-id="232ac-147">标签推理规则的工作原理如下所示：</span><span class="sxs-lookup"><span data-stu-id="232ac-147">The label inference rule works as follows:</span></span> 
- <span data-ttu-id="232ac-148">当且仅当 $P (M = y_2 时，为 _2 $ 赋值 $y $x $ 的示例U ( \theta) x) + b > $0.5 ()  ($y) </span><span class="sxs-lookup"><span data-stu-id="232ac-148">A sample $x$ is assigned label $y_2$ if and only if $P(M=y_2|U(\theta) x) + b > 0.5$  (RULE1) (otherwise it is assigned label $y_1$)</span></span>

<span data-ttu-id="232ac-149">明确 $b $ 必须在时间间隔 $ (-0.5，+ 0.5) $ 才能有意义。</span><span class="sxs-lookup"><span data-stu-id="232ac-149">Clearly $b$ must be in the interval $(-0.5,+0.5)$ to be meaningful.</span></span>

<span data-ttu-id="232ac-150">如果根据 RULE1 为 $x $ $b 推断的标签与 $y $) ，则将定型事例 $ (x、y \in \mathcal{D} $ 视为 *错误分类* 。</span><span class="sxs-lookup"><span data-stu-id="232ac-150">A training case $(x,y) \in \mathcal{D}$ is considered a *misclassification* given the bias $b$ if the label inferred for $x$ as per RULE1 is actually different from $y$.</span></span> <span data-ttu-id="232ac-151">Misclassifications 的总数是分类器的 *定型分数* （给定偏差 $b $）。</span><span class="sxs-lookup"><span data-stu-id="232ac-151">The overall number of misclassifications is the *training score* of the classifier given the bias $b$.</span></span> <span data-ttu-id="232ac-152">*最佳*分类器偏差 $b $ 将定型分数降到最低。</span><span class="sxs-lookup"><span data-stu-id="232ac-152">The *optimal* classifier bias $b$ minimizes the training score.</span></span> <span data-ttu-id="232ac-153">在给定预计算概率估计 $ \{ P (M = y_2 的情况下，很容易就会看到。U ( \theta) x) | (x，\* ) \in\mathcal{D} \} $，通过最多 $ \ log_2 (| \mathcal{D} |) $ 个步骤，可以通过二进制搜索以 "间隔 $ (-0.5，+ 0.5) $" 来找到最佳分类符偏移量。</span><span class="sxs-lookup"><span data-stu-id="232ac-153">It is easy to see that, given the precomputed probability estimates $\{ P(M=y_2|U(\theta) x) | (x,\*)\in\mathcal{D} \}$, the optimal classifier bias can be found by binary search in interval $(-0.5,+0.5)$ by making at most $\log_2(|\mathcal{D}|)$ steps.</span></span>

### <a name="reference"></a><span data-ttu-id="232ac-154">参考</span><span class="sxs-lookup"><span data-stu-id="232ac-154">Reference</span></span>

<span data-ttu-id="232ac-155">此信息应该足以开始播放代码。</span><span class="sxs-lookup"><span data-stu-id="232ac-155">This information should be enough to start playing with the code.</span></span> <span data-ttu-id="232ac-156">但是，如果想要了解有关此模型的详细信息，请阅读原始提议： [ *' 以线路为中心的量程分类器 '、Maria Schuld、Alex Bocharov、Krysta Svore 和 Nathan Wiebe*](https://arxiv.org/abs/1804.00633)</span><span class="sxs-lookup"><span data-stu-id="232ac-156">However, if you want to learn more about this model, please read the original proposal: [*'Circuit-centric quantum classifiers', Maria Schuld, Alex Bocharov, Krysta Svore and Nathan Wiebe*](https://arxiv.org/abs/1804.00633)</span></span>

<span data-ttu-id="232ac-157">除了接下来的步骤中所示的代码示例外，还可以在[本教程](https://github.com/microsoft/QuantumKatas/tree/main/tutorials/QuantumClassification)中开始探索量程分类</span><span class="sxs-lookup"><span data-stu-id="232ac-157">In addition to the code sample you will see in the next steps, you can also start exploring quantum classification in [this tutorial](https://github.com/microsoft/QuantumKatas/tree/main/tutorials/QuantumClassification)</span></span> 
